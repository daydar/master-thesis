\documentclass[11pt,a4paper]{report} 

% Für doppelseitigen Ausdruck (nur bei > 60 Seiten sinnvoll)
% \usepackage{ifthen}
% \setboolean{@twoside}{true}
% \setboolean{@openright}{true} 

\include{preamble} % alle Pakete und Einstellungen

% Hier anpassen 
% \newcommand{\welchethesis}{Bachelor}
\newcommand{\welchethesis}{Master}
\newcommand{\thesisofwas}{of Science}
\newcommand{\titel}{Data Mining komplexer Datenstrukturen aus PDF-Dokumenten}
\newcommand{\kurztitel}{Data Mining PDF Dokumente}
\newcommand{\autor}{Deniz Aydar}
\newcommand{\datum}{25. Juli 2022} % Abgabedatum
\newcommand{\ort}{Wiesbaden}
\newcommand{\referent}{Prof.\ Dr.\ Dirk Krechel}
\newcommand{\korreferent}{Prof.\ Dr.\ Philipp Schaible}

\begin{document}
\include{vorspann} % Titelseite, Erklärungen, etc.

\begin{abstract} 

PDF-Dokumente besitzen viele Informationen, aus denen sich neue Daten generieren lassen können.
Doch das Extrahieren von solchen Daten ist heutzutage immer noch mit Hürden verbunden.
Dies gilt auch für die Dokumente, die in den Prozessen von Autohäusern und deren Kfz-Werkstätten verwendet 
und anschließend gelagert werden.
Um die Frage zu beantworten, inwiefern neue Daten aus dieser Art von Dokumenten verarbeitet werden können, 
wird im Rahmen dieser Arbeit ein System konzipiert und entwickelt, welches es ermöglichen soll, weiteres Wissen zu gestalten 
beziehungsweise zu generieren.
Dieses System will dabei Ansätze aus der künstlichen Intelligenz nutzen und 
eine grafische Schnittstelle für die Möglichkeit der menschlichen Nutzung anbieten.

\end{abstract}

\tableofcontents


\chapter{Einleitung}\label{chap:einf}

„Digitalisierung im Alltag voranbringen“ – Das war einer der Wahlslogans während der Bundestagswahl 2021. 
Gemeint war damit die Forderung nach einer zunehmende Digitalisierung im privaten Alltag vieler Bürger:innen, 
aber auch in der Wirtschaft machte sich wachsend der Wunsch nach mehr digitalen Alternativen breit (vgl. Bundesregierung 2021). 
Dieser Wunsch beinhaltete vor allem einen Wechsel von gängigen Papierformen verschiedenster Dokumente hin
zu denselben in digitaler Ausprägung. 

Genau jenes Bedürfnis nach Digitalisierung betrifft auch die vielen Arbeitnehmer:innen und Händler:innen in Autohäusern 
und deren Kfz-Werkstätten, die durch ihre beruflichen Tätigkeiten mit einer Vielzahl an Unterlagen, 
Dokumenten oder Belegen arbeiten müssen. Unter dieser Vielzahl fallen Dokumente wie Werkstatt-, Kauf- und Mietverträge 
sowie Rechnungen oder Diagnoseberichte. 

Jene Beschäftigte in einigen Autohäusern und deren Kfz-Werkstätten sind Kund:innen bei ilexius GmbH. 
Ilexius bietet Enterprise-Resource-Planning (ERP) Systeme an, mit Hilfe derer Arbeitsaufträge und Arbeitsschritte innerhalb des 
täglichen Arbeitsprozesses in Autohäusern und deren Werkstätten durch Digitalisierung 
vereinfacht werden. In Kooperation von ilexius GmbH werde ich daher die Problematik meiner Thesis, die ich im folgenden
noch genauer beschreiben werde, lösen und in die Tat umsetzen.

Durch eine Digitalisierung jener Dokumente eröffnen sich Vorteile wie bessere Zugänglichkeit, 
größere Langlebigkeit und vor allem eine angepasste und leichtere Nutzung, 
die zu einer höheren Effektivität innerhalb täglicher Arbeitsschritte führt.
Durch bisherige erste Schritte der Digitalisierung sind diese benötigten Dokumente 
bereits elektronisch aufgearbeitet und den Beschäftigten in Autohäusern und deren Kfz-Werkstätten zur Verfügung gestellt worden. 

Die Folge dessen ist, dass alle Dokumente standardisiert sind und dadurch die in den Dokumenten beinhalteten Informationen 
neu verarbeitet werden können. Eine Extraktion der Daten der einzelnen Dokumente ist jedoch nur eingeschränkt möglich, 
da die Struktur im gängigen Gebrauch im Dateiformat Portable Document Format (PDF) festgelegt ist. 

Jene Limitierung der Datenextraktion wirkt sich gleichermaßen auf die Dealer-Management-Systeme (DMS), 
mit solchen die Mitarbeiter:innen ihre Prozesse abwickeln, aus. Der daraus resultierende Arbeitsaufwand, 
welcher dabei entsteht, um die Daten wiederverwendbar zu konstruieren, ist derzeit enorm. 

\section{Zielsetzung}\label{sec:ziel}

Aus diesem Grund möchte ich innerhalb dieser Master-Thesis ein System entwickeln, 
das genau jene Problematik erleichtert und löst.
Innerhalb üblicher Methoden werden heutzutage die Daten zunächst über eine grafische Anzeige mithilfe 
einer bereits existierenden Benutzeranwendung via gängiges Kopieren und Einfügen entnommen. 
Dieser Schritt funktioniert zwar im Regelfall, ist jedoch oftmals stark fehleranfällig und kann lückenhaft sein. 

In so einem Fall muss das Einfügen und Kopieren manuell durchgeführt werden, 
was bei einer riesigen Menge an Dokumente zu einer monotonen Arbeit für die Händler der Autohäuser führt. 
Andernfalls können die Daten eigenhändig abgeschrieben werden – jener Vorgang benötigt jedoch viele Ressourcen.
Eher geeignet ist es, einen Datenkonverter für die PDF Dateien zu nutzen, um so die Dokumente beispielsweise in Bilder umzuwandeln, 
wodurch die Daten zugänglicher sind, aber immer noch verarbeitet werden müssen.
Die letztgenannte Möglichkeit wirkt allerdings eher wie eine Übergangslösung als eine professionelle endgültige Durchführung.

Aufgrund dieser bisherigen teils aufwendigen und mit Fehlern verbundenen Möglichkeiten möchte ich mich in meiner Thesis 
von diesen Optionen abwenden und das Data-Mining nutzen. 
Das Data-Mining soll eine Automatisierung unterstützen, mit der Daten aus dem PDF-Dokument extrahiert werden. 
Allein über das Data-Mining ist es möglich, Inhalte zu extrahieren und für andere Systeme bereit zu stellen. 

Der Unterschied zwischen meiner Verwendung des Data-Minings und zum gängigen Data-Mining 
liegt dabei in den Einschränkungen des Dateiformats: 
der Quelltext einer solchen Datei stellt erstens keine klare Hierarchie der Daten dar und zweitens 
besitzt es durch das Fehlen von Markierungen keine Informationen darüber, 
was die Daten an sich darstellen sollen~\cite{docsumoPDFScraperScrape2022}. 

Mit Hilfe dieser Technik des Data-Minings soll es ermöglicht werden, aktuelle Prozesse detaillierter zu steuern und zu überwachen. 
Des Weiteren können unter anderem Abgleiche von Rechnungen mit dem System durchgeführt werden 
und weitere Datenanalysen und Reporting kreiert werden.
Mit dem derzeitigen Stand der Datenextraktion können jedoch lediglich Agierende aus dem technischen Bereich arbeiten, 
da allein sie das dafür technisch notwendige Know-how besitzen. 
Die Technologien hierfür benutzen unterschiedliche Ansätze und müssen daher zunächst für diesen Fall ausgewählt werden. 

Damit jedoch auch für Agierende innerhalb des technischen Bereichs die Nutzung nicht zu abstrakt bleibt, 
soll durch die Entwicklung eines Systems der Zugriff greifbarer gestaltet werden, 
welches wiederum allein durch eine benutzerfreundliche Anwendung ermöglicht wird. 
Die Bneutzer sollen eine Möglichkeit bekommen und über eine Eingabe der Steuerung bestimmen können, 
welche Informationen extrahiert werden sollen.

Darüber hinaus muss eine Automatisierung vorhanden sein, um auch eine große Datenmenge verarbeiten zu können;
zudem sollen die Vorgänge eine niedrigere Fehlerquote aufzeigen. 
Die Prozesse müssen außerdem während des gesamten Ablaufs über eine Steuerung und Regelung kontrollierbar sein.
Solche Prozesse können zum Beispiel über Regeln festgelegt beziehungsweise gesteuert werden, 
die bei einer Erfüllung weitere Aktionen oder Regeln auslösen können.

Diese beschriebene Eigenschaft kann durch eine symbolische künstlichen Intelligenz (KI) abgedeckt werden, 
welche eine vorgegebene Verarbeitung möglich macht. 
Hiermit bezeichnet man einen altmodischen Ansatz der KI, bei der über das Festlegen von Symbolen 
menschliches Wissen in einer Logik gehalten wird und diese benutzt wird um weiteres Wissen zu generieren~\cite{dicksonWhatSymbolicArtificial2019}.
Durch eine Parametrisierung einer solchen KI kann dann die Unschärfe der ausgewählten Daten festgelegt werden, 
damit eine Feineinstellung stattfinden kann. 
Das heißt, der Benutzer eines solchen Systems soll sich beginnend von groben Definitionen für die Verarbeitung 
zu einer detaillierten und angepassteren Definition über Feedback des Systems hinarbeiten. 

Mit Hilfe dieser Annäherung an Definitionen und Regeln kann so für eine bessere Erfolgsquote gesorgt werden. 
Zudem kann der Ansatz des Machine Learnings (ML), wobei erfolgreiche Verarbeitungen einem System antrainiert werden, 
weitere Dokumente aus Daten extrahieren. 
Hierbei ist noch offen, welcher Typ des Machine Learnings sich für diesen Fall eignet.

Beide Ansätze – der der symbolischen KI und der des Machine Learnings – bieten als Option die Entwicklung 
einer grafischen interaktiven Entwicklungsumgebung (IDE) an. 
Jene soll das Festlegen von Definitionen und Regeln erlauben, mit welchen die Dokumente verarbeitet werden können. 
Außerdem soll diese Oberfläche verschiedene Funktionalitäten anbieten und auch Feedback sowohl bei einem problemlosen Lauf 
als auch bei einem fehlerhaften Lauf zurückgeben. 

Des Weiteren soll das System es ermöglichen Änderungen der Definitionen anzumerken und Unterschiede zu erstellen, 
womit auch bisherige Ergebnisse angezeigt werden.
Die Festlegung von Definitionen und Regeln soll außerdem durch eine Ansicht der Dokumente unterstützt werden.
Insgesamt wird das System die zwei Ansätze als Subsysteme aufteilen um die Umsetzung zu modularisieren 
und einen Vergleich zu ermöglichen.

\section{Ablauf}\label{sec:ablauf}

Um genau dieses optimale System für das beschriebene Szenario entwickeln zu können, werde ich in dieser Arbeit wie folgt vorgehen: 

Für eine vollständige Auseinandersetzung soll eine weitere Analyse stattfinden,
damit die vorhandenen Grundlagen für den Anwendungsfall evaluiert werden können. 
Das heißt, es werden mit Hilfe der Funktionalitäten der Bibliotheken Ergebnisse 
auf Basis der Datensätze erzeugt, um die Erfolgsquoten zu überprüfen. 
Diese werden dann verglichen und ausgewertet. Parallel dazu soll die Umgebung 
für die Entwicklung eingerichtet werden, mit der die Implementierung der Systeme stattfinden soll. 

Danach widme ich mich der Konzeption und der Entwicklung des Systems.
Diese beginnt mit der symbolischen KI als ein Subsystem des gesamtem Projekts, sodass ein direkterer Ansatz ermöglicht wird. 
Die Entwicklung hierbei wird im Wasserfall-Ansatz, einem Ansatz bei der phasenweise 
die Eigenschaften einer solchen KI umgesetzt werden, 
um eine Grundlage für die nächste Komponente zur Verfügung zu stellen.

Bei dieser Komponente handelt es sich um die interaktive Entwicklungsumgebung, die den Zugang für den Benutzer zum System darstellt.
Deswegen folgt im nächsten Schritt die Konzeptionierung und Entwicklung der interaktiven Entwicklungsumgebung, 
welche sich während der Entwicklung der symbolischen KI parallelisieren lässt. 
Sobald das Gerüst der Benutzeroberfläche fertiggestellt ist, möchte ich dies mit der symbolischen KI verbinden.

Als Gegenstück wird sich dann mit dem Machine Learning Subsystem, 
bei der gleichermaßen eine Konzeptionierung und Entwicklung stattfindet, gewidmet.
Hier wird das bisherige Gesamtkonstrukt in einem iterativen Ansatz umgesetzt, 
sodass der Hauptteil so früh wie möglich verfügbar ist, 
damit das Training des ML-Systems durch die Dokumente auch zeitnah starten kann. 
Auf das Auswerten dieses Subsystems folgt eine Anpassung dessen. 

Danach wird das Software Testing für die beiden Subsysteme eingeführt. 
Mit Unit Testing, Integration Testing und Functional Testing wird das erfolgreiche Ausführen gewisser Eigenschaften abgedeckt.
Dies dient wiederum als Grundlage dafür, die Continuous Integration (CI) für das System einzuführen, 
womit eine sichere und konsistente Entwicklung nach dem Prototyping angeboten wird.  

Wenn dann aber die Systeme gegeben sind, kann das grafische Tool weiter angepasst werden, welches 
sich dann mit den Systemen verbinden soll, um die Ausführbarkeit der Funktionalitäten zu überprüfen. 
Um ein gemeinsames System zu ermöglichen, welches dann gegebenenfalls weitere Testschritte bedarf,
werden dann die Komponenten zusammengeführt.  
Sobald jenes eine erfolgreiche Form annimmt, wird der Teil der Continuous Integration mit einbezogen, 
mit jener das System einen Zustand der Instandhaltung annehmen kann. 
Über Testing sollen so mögliche Fehlerquellen entdeckt werden, bevor diese überhaupt auftreten können.

Unter die Zielsetzung für meine beiden eben genannten Ansätze fallen die Kundendaten mit den Eigenschaften wie 
Name, Firma, Adresse und Kundennummer, die Fahrzeugdaten worunter Fahrgestellnummer, Modell, 
Farbe, Motor, Erstzulassung, Letzter/nächster Service und TÜV fallen und 
die Abrechnungsdaten wie die Jobs mit Arbeitspositionen und Teilen, Preise etc. 

Dazu stehen als weitere Grundlage mehr als 10.000 annotierte unterschiedliche Dokumente mit genau den Daten, 
die extrahiert werden sollen, als Datenbank zur Verfügung. 
In Zukunft sollen für das Mengengerüst mehr als 100.000 Dokumente pro Jahr bearbeitet werden. 
Hierbei ist es aber auch möglich, aus den bereits bestehenden Daten einen Pool für den Lernprozess bzw. 
für die Verifikation zu bilden.

Zum Schluss werde ich die Ergebnisse des gesamten Systems betrachten und einen Ausblick zu der Thematik geben.

\chapter{Hintergrund}\label{chap:hint}

Um die Problematik detaillierter darzustellen und um eine Übersicht zu ermöglichen, 
wird in diesem Kapitel der Rahmen der Problematik aus technischer Sicht genauer dargestellt.
Hierbei werden die möglichen Technologien abgewogen und evaluiert. 
Zunächst jedoch wird der Kontext ausführlicher erklärt.

\section{Kontext}\label{sec:kon}

Im Automobilbereich wird der Verkauf von Kraftfahrzeugen in zwei Segmente aufgeteilt: dem Sales-Bereich, 
bei dem es sich um den Verkauf 
von Neu- und Gebrauchtwagen handelt und dem After-Sales-Bereich, bei jenem eine Bindung zum Kunden über den Verkauf von 
Verschleiß- und Ersatzteilen geschaffen wird.~\cite{theuererWasMachtSales}
Diese Aufteilung der Segmente wird ebenfalls von den Autohäusern angewendet, zugleich werden für diese 
jeweils gängige Abläufe als Prozesse benutzt.

Für den After-Sales-Bereich bietet ilexius GmbH zwei Arten von ERP-Systeme an. 
Die erste Art ist das von der Automobilmarke Jaguar Land Rover finanzierte `vTab', welches eine Plattform 
für elektronische Fahrzeugkontrollen 
und ein Backend für Jaguar Land Rover Mitarbeiter:innen, mit dem die Autohäuser bewertet werden können, anbietet.
Die zweite Art mit dem Namen `ATT' ist eine Plattform. 
Mit derjenigen können Autohäuser ihre Arbeitsprozesse in Schrift, Sprache und Bild dokumentieren.
Die Dokumentation erfolgt mittels mobiler Endgeräte und werden an das ERP-System weitergeleitet, 
sodass die Dokumentation archiviert werden kann.

Die ERP-Systeme, die bereits im Kapitel~\ref{sec:ziel} beschrieben worden sind, unterstützen 
damit die Kunden:innen von ilexius GmbH 
in den Autohäusern und deren Werkstätten mit der Planung, der Steuerung sowie der Verwaltung 
von verschiedenen Aufgaben in ebenjenem After-Sales-Bereich.

Eine Art der Aufgaben sind Arbeitsaufträge, bei der es sich in den meisten Fällen um die Behandlung 
eines Autos inklusive Kundendaten handelt. 
Der Auftrag besteht hierbei aus einer Liste an Arbeitsabläufen, die als Jobs bezeichnet werden. 
Die Jobs wiederum unterteilen sich in einzelne Arbeitsschritte, die Joblines genannt werden, 
welche die konkreten Verrichtungen der Arbeit darstellen und somit den gesamten Arbeitsauftrag 
in einzelne Teilschritte vervollständigen.

Sind diese Aufträge abgeschlossen, benutzen die Mitarbeiter:innen der Autohäuser ihre Dealer-Management-Systeme,
 um eine Rechnung zu erstellen und diese im ERP-System zu archivieren.
Das ERP bietet des Weiteren eine Schnittstelle für diese Dokumente, wie z.B die Rechnungen oder die Garantieaufträge an, 
die allein über Scanner die Dokumente übertragen. 
Dort werden die Dokumente verarbeitet und mit Hilfe eines OCR Scanners digitalisiert. 

Aktuell ist allerdings die Digitalisierung nicht in der Lage, einen Kontext beziehungsweise (bzw.) 
eine Semantik für diese Dokumente zu erstellen.
Idealerweise kann dies genutzt werden, um aus den Dokumenten Arbeitsaufträge zu generieren und somit zu digitalisieren.
Demzufolge ergibt sich hier an der Stelle die Möglichkeit, einen neuen Ansatz zu testen, sodass die Informationen 
aus diesen Dokumenten als Daten erhaltbar sind.

\section{Analyse}\label{sec:ana}

Das Extrahieren von Daten ist eine gängige Methode um Informationen aus verschiedenen Systemen wie Datenbanken 
oder Software as a Service (SaaS) Plattformen zu beschaffen~\cite{stichdataWhatDataExtraction}. 
Durch diese Beschaffung können die Daten weiter verarbeitet werden um neue Lösungsmöglichkeiten für das eigene System anzubieten.
Die Arten des Extrahierens unterscheiden sich hierbei im Zeitverlauf und der Herangehensweise. 
So können Diese extrahiert werden sobald Änderungen der Daten beobachtet oder mitgeteilt werden, 
wodurch die Daten nach und nach beschafft werden.

Des Weiteren gibt es die Option eine komplette Extraktion durch zu führen. 
Dieser Ansatz kann sich in vielen Fällen als nachteilig erweisen, da die Daten jedes mal 
bei Anpassungen der ursprünglichen Information neu verarbeitet werden müssen.
Da sich die Problematik im Rahmen der Thesis auf Dokumente, die bereits im ERP-System archiviert  wurden, 
bezieht, trifft dieser Fall hier nicht zu.

\subsection{Extraktionsbibliotheken}\label{sec:exwerk}

Für das Data Mining von PDF Dateien existieren bereits Lösungen, die von großen Unternehmen wie Amazon Web Services (AWS) 
oder Adobe bereits angeboten werden. 
So nutzen Produkte wie Textractor~\cite{amazonIntelligenteExtraktionText} von AWS oder 
Adobes Extractor API~\cite{developersExtractTextPDF} Künstliche Intelligenz, Machine Learning und Optical Character Recognition (OCR) 
um die Extraktion der Daten zu ermöglichen. 
Unternehmen wie ABBYY~\cite{abbyyPDFSoftwareOpen} habe sich bereits auf diesem Gebiet spezialisiert 
und sind erfolgreich mit der Datenextraktion. 
Auch existieren bereits Open-Source Bibliotheken, die sich mit dem Entnehmen der Daten aus PDF Dokumenten beschäftigen.

Die Meisten davon lassen sich hinsichtlich der Aufteilung der Aufgaben gruppieren. 
So erlauben unter anderem PDFMiner~\cite{unixuserPDFMiner} oder Apache Tika~\cite{apacheApacheTikaApache}, 
welches vor der Extraktion der Texte und Metadaten noch die Klassifizierung und Identifizierung des Medientyps von Dateien 
anhand einer umfangreichen Hierarchie~\cite{mattmannTikaAction2012} besitzt, Texte aus PDF Dokumenten zu gewinnen. 
Funktionalitäten für das Extrahieren von Daten aus Tabellen bieten Projekte wie 
Tabula~\cite{tabulapdfTabulapdfTabulaTabula} oder Camelot~\cite{camelotCamelotPDFTable2022}
an womit ein weiterer Aspekt durch die Nutzung abgedeckt wäre. 

Bei Beiden zeigen sich niedrige Fehlerquoten auf~\cite{atlanhqComparisonOtherPDF} und die Nutzung von einer Fuzzy Logik. 
Camelot ermöglicht dazu noch ein Web Interface womit die simple Extraktion über einen Browser lokal stattfinden kann.
Andere Bibliotheken wie Textricator~\cite{measuresMeasuresforjusticeTextricatorTextricator} sind umfangreicher gestaltet 
und ergeben geeignete Schnittstellen für die Anfragen spezifischer Inhalte. 

Doch auch wenn dies mit textbasierten Dokumenten funktioniert und der Export flexibel ist, 
können zunächst weder nicht-textbasierte noch komplexere PDF Dokumente verarbeitet werden. 
Des Weiteren sind die Verarbeitungen nicht automatisierbar und die Nutzergruppe 
wird durch eine fehlende grafische Komponente eingeschränkt.
Aus dieser Problematik kommend, zeigt sich, dass community-angetriebene Projekte wie PdfMiner.six~\cite{pdfminerWelcomePdfminerSix}, 
welches eine Abspaltung des bereits erwähnten PdfMiner ist, und PdfPlumber~\cite{singer-vinePdfplumber2022}, 
welches wiederum auf PdfMiner.six basiert, als geeignete Alternativen darstellen. 

PdfPlumber stellt im Vergleich zu seinem Ausgangsprojekt dazu noch eine Tabellenextraktion bereit und bietet für beide Aspekte 
der Extraktion eine Schnittstelle an. 
Es hält eine Kommandozeile, die Möglichkeit, visuelles Debugging zu nutzen, 
und Funktionalitäten wie das Entfernen von Duplikaten in Texten, bereit.

Das visuelle Debugging, welches in Abbildung~\ref{fig:vd} zu sehen ist, ermöglicht es unter anderem, 
die Texte des Dokuments hervorzuheben, was wiederum die Extraktion greifbarer darstellt 
und die Entwicklung mit den Funktionalitäten unterstützt.

In dieser Abbildung sind die persönlichen Daten des Auftrags verpixelt und damit anonymisiert.
Auch die Funktionalität des Entfernens von Duplikaten kann dann zum Einsatz kommen, wenn Dokumente bei ihrer Erstellung 
oder Verarbeitung versehentlich die selben Einträge nochmal im Quellcode des PDFs besitzen. 

\begin{figure}[htb]
    \centering
    \subfloat[ohne visuellem Debugging]{\label{fig:wovd}
    \frame{\includegraphics[width=.45\textwidth]{images/kfz3000_werkstattauftrag_pixelated.png}}}
    \subfloat[mit visuellem Debugging]{\label{fig:wvd}
    \frame{\includegraphics[width=.45\textwidth]{images/kfz3000_werkstattauftrag_vd_pixelated.png}}}    
    \caption{visuelles Debugging bei einem Dokument }
~\label{fig:vd}
\end{figure}

Für die Auswahl einer Extraktionsbibliothek ist auch die dazu gehörige Programmiersprache für das System der Datenextraktion relevant, 
in der die Bibliothek umgesetzt wurde. 
Hierbei fällt das für die bereits genannten Bibliotheken diese sich in zwei Sprachen gruppieren lassen.
Projekte wie Tika, Tabula und Textricator sind mit Hilfe von Java implementiert, 
während die restlichen aufgezählten Bibliotheken in Python geschrieben sind.
Für Tabula gilt hier die Ausnahme, dass es für das Projekt unterschiedliche Wrapper für weitere Programmiersprachen wie Python gibt.
Hieraus ist zu beobachten, dass die Nutzung von Python viele mögliche Extraktionswerkzeuge erlaubt, 
welches sich für die spätere Umsetzung als noch bedeutsam erweist.

Die Hürden für eine komplett automatische Extraktion bestehen trotzdem noch.
So ist nämlich nicht klar, wie das zu erwartende Verhalten der Logik der Extraktion für den generellen Fall auszusehen hat.
Eine differenzierte Extraktion für weitere Eigenschaften eines Dokuments wie die Zeilenumbrüche von Absätzen, 
die Seitennummern, die Kopf- und Fußzeilen, die Formatierung et cetera (etc.) ist daher nötig.
Die Frage, die sich dabei stellt ist, welche Eigenschaften eines Dokuments genau zusätzlich extrahiert werden sollen. 
Um eine Auswahl zu treffen, müssen die Arten der Dokumente, die in dem ERP-System auftreten, genauer durchleuchtet werden.

\subsection{Dokumentarten}\label{sec:dokart}

Bei den Dokumenten, die in das ERP-System eingescannt werden, handelt es sich in der Mehrheit um Garantie- und Werkstattaufträge sowie Rechnungen. 
Jedes Autohaus benutzt hierbei ein individuelles Muster bzw. Format für die Arten der Dokumente.
Jedoch ist der Aufbau dieser Dokumente zu Teilen indirekt durch die von den Autohaus genutzten Dealer-Management-Systeme vorgegeben.

Dadurch passiert es, dass es Autohäuser gibt, welche das gleiche DMS verwenden 
und sowohl das selbe als auch ein in Detail angepassten Dokumentenaufbau für ihre Aufträge benutzen. 
Dem Umfang dieser Thesis geschuldet werde ich mich daher auf drei Dokumententypen von unterschiedlichen Autohäusern 
mit ihren jeweils genutzten DMS beschränken und anhand jener drei Typen mein Vorgehen exemplarisch demonstrieren.
Die Abbildungen der Dokumente in diesem Abschnitt zeigen zu Teilen persönliche Daten 
und wurden deshalb für die Thesis durch Verpixelung der sensitiven Felder anonymisiert.

In Abbildung~\ref{fig:care_g} wird die erste Art eines Auftrags dargestellt.
Bei diesem digitalen Schriftstück handelt es sich zunächst von der Form her um eine Garantierechnung des Dealer-Management-Systems Care\@.
Das Dokument besitzt hierbei drei Bereiche, die relevante Informationen für einen Arbeitsauftrag enthalten.
In der oberen Hälfte befindet sich zunächst das Adressfeld, welches die Kundeninformationen im genormten Adressformat enthält, 
und ein weiterer Abschnitt, welches die Fahrzeugdaten festhält.

\begin{figure}[htb]
    \centering
    \frame{\includegraphics[width=.7\textwidth]{images/care_garantieauftrag_pixelated.png}}
    \caption{Exemplar eines Garantieauftrags}
~\label{fig:care_g}
\end{figure}

Hierbei ist zu beobachten, dass dieser Abschnitt sich mit einer Tabelle vergleichen lässt, 
da die einzelnen Einträge mit jeweils unterschiedlich großen Abständen verteilt dargestellt werden 
aber gleichzeitig noch eine Gesamthöhe pro Spalte beziehungsweise eine Gesamtbreite pro Höhe einer Tabelle erfüllen.
Jedoch fällt auf, dass sich, im Gegensatz zur Anzahl der Zeileneinträge, die Anzahl der Spalteneinträge pro Zeile unterscheidet.

Außerdem fehlen jegliche Einrahmungen beziehungsweise jegliche Trennlinien für die Zeilen und die Spalten.
Bei den Einträgen in diesem Abschnitt handelt es sich um die Abrechnungsnummer, das Datum der Reparatur, 
den Fahrzeugmodellnamen, das Kfz-Kennzeichen, den zuletzt abgelesen Kilometerzählerstand, der Arbeitsauftragsnummer, der Fahrzeug-Identifikationsnummer beziehungsweise der
`Vehicle Identification Number' (VIN), des zuständigen Autohaus Mitarbeiters und das Datum der Erstellung des Arbeitsauftrags.
Verpixelt und damit anonymisiert wurden die Felder des Kfz-Kennzeichens, der VIN und des zuständigen Mitarbeiters.

Dagegen wird im unteren Bereich eine Tabelle durch die Aufzählung der einzelnen Jobs und Joblines dargestellt.
Die Tabelle ist an der Kopfzeile zu erkennen, welche die Jobs und Joblines in die Arbeitsnummer beziehungsweise in die Teilenummer, 
in der Bezeichnung, in die Menge des Arbeitszeitwertes, in den Preis und in den Betrag übersichtlich aufteilt.
Der Arbeitszeitwert ist dabei eine Einheit, die für eine genaue Abrechnung der Arbeitszeiten, 
um eine Aufteilung der Stunden zu ermöglichen.~\cite{BedeutungAWArbeitswert}
Die Menge wird hierbei als Reparaturzeit oder Anzahl angegeben.
AUßerdem existiert b<ei dieser Tabelle nur für die Kopfzeile eine untere Trennlinie.
Auch hier sind Probleme bezüglich (bzgl.) der Form der Tabelle zu erkennen.

Während die ersten vier Zeilen noch unauffällig erscheinen, besitzt die fünfte Zeile 
bei der Bezeichnung einen längeren Text, sodass der restliche Text in weiteren Zeilen in der Spalte der Bezeichnung 
darunter geschrieben steht.

Während die Abstände zwischen den Spalten insgesamt konsistenter erscheinen, sind in der Abbildung~\ref{fig:care_g} 
in mehreren Zeileneinträgen Überläufe bei den Arbeitsnummern bzw.\ den Teilenummern zu erkennen.
In der letzte Zeile ist außerdem zu erkennen, dass die Einträge zu Teilen auch verschoben dargestellt werden, 
was eine eindeutige Erkennung der Positionierung noch weiter erschwert.
Inhaltlich werden Jobs mit den dazu gehörigen Joblines darunter folgend dargestellt. 

Dies ist durch die Codierung der Arbeitsnummern bzw.\ der Teilenummer zu erkennen 
und die Hierarchie ist durch das jeweilige Dealer-Management-System vorgegeben.
In der ersten Spalte können daher Codes von Jobs, Reparaturzeiten, Ersatzteilen, 
Fremdleistungen, Stempelzeiten und Kundenbeschwerden auftreten.

Für die Codierung ist keine standardisierte Spezifikation vorhanden, weswegen dies für jedes Autohaus 
und deren Nutzung der DMS individuell definiert.
So sind die Codes der Jobs als eine fünfstellige Zeichenkette aus Ziffern definiert, welche mit den Ziffern `00' anfangen müssen
und die Reparaturzeiten besitzen eine sechsstellige Zeichenkette aus Ziffern.
Unterhalb der Tabelle werden am Ende des Dokuments noch die gesamten Kosten summiert aus den Betragseinträgen dargestellt, 
welche wiederum sich in den Kosten der Arbeit und der Teile aufteilen lassen.

\begin{figure}[htb]
    \centering
    \frame{\includegraphics[width=.7\textwidth]{images/formel1_rechnung_pixelated.png}}
    \caption{Exemplar einer Rechnung}
~\label{fig:formel1_r}
\end{figure}

In Abbildung~\ref{fig:formel1_r} wird eine Rechnung der Formel 1 DMS dargestellt.
Auch hier sind sämtliche personenbezogenen Daten verpixelt und anonymisiert worden.
Hier ist im Vergleich zur vorherigen Abbildung~\ref{fig:care_g} ein ähnlicher Aufbau ersichtlich.
Während das Adressfeld gewöhnlich erscheint, sind jedoch die Fahrzeug- und Kundendaten diesmal 
in zwei unterschiedlichen Tabellen aufgeteilt.

Die Erste von diesen Tabellen befindet sich im oberen Teil der rechten Seite des Dokuments, 
in welchen die einzelnen Kundeninformationen aufgelistet sind.
Die Tabelle für die Fahrzeuginformationen befindet sich im unteren Bereich der oberen Hälfte des Dokuments 
und erweist sich durch das Vorhandensein der Trennlinien als eine klassische Tabelle.
Im letzten Teil der Formel 1 Rechnung befindet sich die Liste der Job und Joblines.

Hier fallen ähnliche Problematiken, wie die unterschiedlichen Abstände zwischen den Spalten der Einträge, 
im Vergleich zu dem Care Garantieauftrag auf.
Auch existieren Überläufe der Einträge, allerdings treten diese nur für die Felder der Bezeichnung auf.
Die Gesamtkosten werden dann am Ende des Dokuments in den Teilsummen der Beträge angezeigt.

\begin{figure}[htb]
    \centering
    \frame{\includegraphics[width=.7\textwidth]{images/kfz3000_werkstattauftrag_pixelated.png}}
    \caption{Exemplar eines Werkstattauftrags}
~\label{fig:kfz3000_w}
\end{figure}

Die letzten Dokumentart wird in der Abbildung~\ref{fig:kfz3000_w} dargestellt.
Hier handelt es sich um einen Werkstattauftrag von dem Dealer-Management-System KFZ3000, 
welches auch wie die vorherigen Abbildungen anonymisiert wurde.
Dieser Auftrag lässt sich von der Aufteilung der Segmente mit dem Care Warranty Dokument geeigneter vergleichen.
Während auch das Adressfeld sich wieder normiert darstellt, werden Kunden- und Fahrzeugdaten 
über Schlüssel-Wert Anordnungen in einem größeren Feld dargestellt.

Die untere Hälfte des Werkstattauftrags besitzt auch wie die bereits anderen genannten Dokumente 
eine tabellenähnliche Auflistung der Job und Joblines. 
Auch hier sind wieder Überläufe und unterschiedliche Abstände in dem Bereich zu erkennen.
Dazu kommt, dass, im Gegensatz zu den anderen Dokumentarten, 
zwischen den Zeilen ein Art Summenstrich hinzugefügt wird, der die Anzahl der Jobs anzeigen soll.
Außerdem wird im unteren Bereich des Dokuments nur die gesamte Betragssumme dargestellt.

Insgesamt lässt sich feststellen, dass alle vorgestellten Dokumentarten mehrere Hürden 
als gemeinsame Problematik verbindet. 
Die dargebotene Darstellung der Kunden- und Fahrzeuginformationen wird entweder in einer Tabellenform 
gemeinsam oder in zwei verschiedenen Tabellen abgebildet.
Diese Tabellenform besitzt außerdem selten Trennlinien, welche geeignet sind, um eine derartige Tabelle als solche definieren zu können.
Für die Job und Joblines tritt das selbe Problem auf.

Diese Auflistung wird nur durch ihre Anordnung der Einträge und deren Abstände dazwischen als solche erkenntlich, 
bricht in den prototypischen Dokumenten aber mindestens einmal die Form durch Überläufe oder Verschiebungen der Texte.
Es ist festzustellen, dass unterschiedliche Tabellenstrukturen zu unterschiedlichen
Dieses Angelegenheit besteht, da es für Tabellen keine universelle Standardisierung existiert.

Damit diese Formen durch eine Logik des Extrahierens abgedeckt werden, scheint die Möglichkeit der Fallunterscheidung zu bestehen.
Bei dieser Unterscheidung sollen die Dokumente anhand ihrer Arten konzipiert und definiert werden, 
sodass die Entnahme detaillierter stattfinden kann.
Für andere Bereiche eines Dokuments wie dem Adressfeld werden gewöhnliche Funktionalitäten der Extraktion genutzt und zur Verfügung gestellt.
Diese können dann von allen weiteren Dokumentarten geteilt und genutzt werden, 
womit eine Verallgemeinerung verschiedener Dokumententypen ermöglicht wird.


\subsection{Frameworks}

Damit das System für den Prozess der Extraktion als solches sich zeitnah im Rahmen einer Thesis umsetzen lässt, 
können Frameworks dem ganzen Zeitfenster entgegenkommen.
Hierbei werden sich für die Teilsysteme verschiedene Technologien angeschaut und in Erwägung gezogen. 

So wird zunächst die symbolische künstliche Intelligenz dem ersten Teilsystem zugeordnet und in seiner Art unterschieden. 
Zudem müssen die Realisierungsmöglichkeiten der symbolischen KI betrachtet 
und aus den verschiedenen Arten eines wissensbasiertes Systems ein Modell ausgewählt werden.

Ein solches wissensbasiertes System ist hierbei der Oberbegriff für Programme, die mit Hilfe einer Wissensbasis Mechanismen der Schlussfolgerung nutzen, 
um Lösungen für Probleme zu finden~\cite{embreySkillRuleKnowledge}. 
Es existieren verschiedene Modelle eines wissensbasiertes Systems, die sich, je nach Anwendungsfall, als passend erweisen.
So gibt es den Ansatz des regelbasierten Systems bzw. Produktionssystems, bei dem die Schlussfolgerung über Regeln und Fakten, 
die von einem Benutzer definiert werden, gestaltet wird.

Bekannt für die Erstellung eines solchen Systems, ist das in der Programmiersprache C geschriebene Werkzeug CLIPS\cite{clipsCLIPSToolBuilding}, 
welches über eine eigene Syntax für die Regeln verfügt.
Diese Umsetzung wird auch mit dem Wrapper `clipspy' für die Programmiersprache Python zur Nutzung bereitgestellt.
Andere Umsetzungen konzentrieren sich im Vergleich eher auf die Inferenzmaschine, die auch als Regelinterpreter gilt.
Projekte wie `Pyke'~\cite{frederiksenWelcomePyke}, `Durable Rules'~\cite{ruizDurableRules2022} oder `Experta'~\cite{perezNilp0interExperta2022}, 
dem Nachfolger von einem Expertensystem Framework namens `pyKnow', erlauben es eine Regelmenge zu definieren 
und die einzelnen Regeln mit selbstdefinierten Funktionen zu koppeln.

Für die Benutzeranwendung, die die Schnittstelle zum System darstellen soll, 
eignen sich sowohl Benutzerschnittstellen Bibliotheken für den Desktop als auch für den Web.
So können native Graphische Benutzer


\chapter{Konzeption und Umsetzung}\label{chap:konundum} %KAPITEL

Für den Sprung zur Umsetzung bedarf es als Nächstes die Konzeption des technischen Rahmens. 
Danach wird der Verlauf der Entwicklung dargestellt und die 


\section{Aufbau}



\section{Symbolische KI}

Wie im Segment 

\section{Entwicklungsumgebung}

Für die graphische Schnittstelle der Benutzer des Systems

\section{Machine Learning}

Während 

\chapter{Ergebnisse}\label{chap:erg}




\chapter{Konklusion und Ausblick}\label{chap:auf}
 
Im Rahmen der Abschlussarbeit wurde ein System konzipiert und umgesetzt, 
welches eine Extraktion von Dokumenten über zwei Ansätze ermöglicht.
Über die Teilsysteme wurden technische Funktionalitäten eingeführt werden um dem Kapitän die Webanwendung als PWA anzubieten.

Insgesamt eignen sich Webanwendungen um Ressourcen übersichtlicher darzustellen und mobil zu beobachten.
Auch können Offline-Zustände dem Benutzer trotzdem erlauben Aspekte der Anwendung weiter zu benutzen.
Aus den Konzepten und der Umsetzung ergeben sich viele weitere Ideen und Ansätze, die realisierbar sind und implementiert werden können.  

\newpage

% Listen wenn überhaupt ans Ende und nicht an den Anfang.
% Meist ist das aber unnötig.
%\listoffigures % Liste der Abbildungen 
%\listoftables % Liste der Tabellen
% \newpage

\bibliography{thesis}
\bibliography{online}
\bibliographystyle{plain} % Literaturverzeichnis
\begin{btSect}{thesis} % mit bibtopic Quellen trennen
\section*{Literaturverzeichnis}
\btPrintCited{}
\end{btSect}
\begin{btSect}{online}
\section*{Online-Quellen}
\btPrintCited{}
\end{btSect}
% dann mit "bibtex thesis1" und "bibtex thesis2" arbeiten

\end{document}
;;; Local Variables:
;;; ispell-local-dictionary: `de_DE-neu'
;;; End:
